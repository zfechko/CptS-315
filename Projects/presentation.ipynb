{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Detecting Phishing Websites Using Machine Learning\"\n",
    "author: \"Zach Fechko\"\n",
    "format:\n",
    "    revealjs:\n",
    "        theme: blood\n",
    "        transition: slide\n",
    "        width: 1920\n",
    "        height: 1080\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation {.nonincremental}\n",
    "\n",
    "![](https://learn.microsoft.com/answers/storage/attachments/8587-tomsn.png){height=70%, width=70%, .center}\n",
    " \n",
    "- As someone who uses the internet on a daily basis, I've gotten my fair share of phishing emails.\n",
    "- I wanted to see if there was a way to detect \"phishy\" websites using machine learning.\n",
    "- Having a tool that tells you if a website is a phishing website would be a huge benefit to both individuals and organizations.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "::: {.notes}\n",
    "My motivation for this project was that because I'm online on a daily basis I tend to get a good amount of phishing emails from time to time, and also when I was a kid I would \"help\" my parents with phishing training for their jobs as a way to build digital awareness at a young age. The skill of knowing if something is a phishing attack or not is a very important skill to have especially during your professional career, which is why large companies devote resources to training their employees on how to look for it. The idea of eliminating phishing training because a tool exists to classify those websites for users would be a huge benefit to both individuals and large corporations\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Problem and technical approach\n",
    "The goal of this project is to find the optimal machine learning model that can detect phishing websites with the highest accuracy possible based on the features of the website and its URL.\n",
    "\n",
    "**Technical approach**  \n",
    "\n",
    "1. Gather and preprocess dataset\n",
    "2. Train each model on training set and test them on the testing set\n",
    "3. Gather accuracies and store for later comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.notes}\n",
    "The goal for this project is to find the most accurate machine learning model that can classify a website as \"phishy\" or legitimate based on its features. In order to do this I gathered and preprocessed the dataset and split it into a training/testing set. Fit each model with the training set and obtain accuracy ratings from the testing set, then store those accuracies to be compared later\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website Data{.smaller}\n",
    "\n",
    "- The dataset comes from Kaggle\n",
    "- It contains the domain of, and information about the features of 10000 websites and classifies them as either phishy (1) or legitimate (0)\n",
    "\n",
    ". . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Have_IP</th>\n",
       "      <th>Have_At</th>\n",
       "      <th>URL_Length</th>\n",
       "      <th>URL_Depth</th>\n",
       "      <th>Redirection</th>\n",
       "      <th>https_Domain</th>\n",
       "      <th>TinyURL</th>\n",
       "      <th>Prefix/Suffix</th>\n",
       "      <th>DNS_Record</th>\n",
       "      <th>Web_Traffic</th>\n",
       "      <th>Domain_Age</th>\n",
       "      <th>Domain_End</th>\n",
       "      <th>iFrame</th>\n",
       "      <th>Mouse_Over</th>\n",
       "      <th>Right_Click</th>\n",
       "      <th>Web_Forwards</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tobogo.net</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teat09.com</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depositphotos.com</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>superuser.com</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>web.de</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Have_IP  Have_At  URL_Length  URL_Depth  Redirection  \\\n",
       "Domain                                                                    \n",
       "tobogo.net               0        0           1          2            0   \n",
       "teat09.com               0        0           0          3            0   \n",
       "depositphotos.com        0        0           1          1            0   \n",
       "superuser.com            0        0           1          3            0   \n",
       "web.de                   0        0           1          6            0   \n",
       "\n",
       "                   https_Domain  TinyURL  Prefix/Suffix  DNS_Record  \\\n",
       "Domain                                                                \n",
       "tobogo.net                    0        0              0           0   \n",
       "teat09.com                    0        0              0           0   \n",
       "depositphotos.com             0        0              0           0   \n",
       "superuser.com                 0        0              0           0   \n",
       "web.de                        0        0              0           0   \n",
       "\n",
       "                   Web_Traffic  Domain_Age  Domain_End  iFrame  Mouse_Over  \\\n",
       "Domain                                                                       \n",
       "tobogo.net                   1           0           0       0           0   \n",
       "teat09.com                   1           0           0       0           0   \n",
       "depositphotos.com            1           0           1       0           0   \n",
       "superuser.com                1           0           1       0           0   \n",
       "web.de                       1           1           1       0           0   \n",
       "\n",
       "                   Right_Click  Web_Forwards  Label  \n",
       "Domain                                               \n",
       "tobogo.net                   1             0      0  \n",
       "teat09.com                   1             0      1  \n",
       "depositphotos.com            1             0      0  \n",
       "superuser.com                1             0      0  \n",
       "web.de                       1             0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.set_index(\"Domain\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dataset is broken up into 3 main chunks\n",
    "    - Address bar based features\n",
    "    - Domain based features\n",
    "    - HTML/JavaScript based features\n",
    "\n",
    "- I randomly sampled the dataset into a training and testing set along an 80/20 split\n",
    "    - Training set made up of 8000 entries\n",
    "    - Test set made up of 2000 entries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.notes}\n",
    "The dataset I used I came from Kaggle, it contains the domain name and information about the features of 10000 websites, 5000 of them are phishing websites and the other 5000 are legitimate. The dataset is broken up into 3 main chunks, address bar based, domain based, and html/js based features. Using this dataset I shuffled the entries around and then split the data into an 80/20 split, 8000 entries used for training and 2000 for testing.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Used\n",
    "::: {.nonincremental}\n",
    "- Decision Tree\n",
    "    - Used multiple depths and found that `maxdepth = 5` was the most optimal\n",
    "- Random Forest\n",
    "    - Used multiple depths and found that `maxdepth = 5` gave the best results\n",
    "- Binary Logistic Regression\n",
    "    - Used 1000 iterations\n",
    "- SVM\n",
    "    - Used Linear kernel with regularization parameter `C = 1`\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.notes}\n",
    "Due to the dataset using boolean values of 0 and 1 to tell if a website is phishy or not, this is a classification task. The four models that i decided to use are...\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation & Results{.smaller}\n",
    "- Each model is fit on the training set and then evaluated on the testing set, where its accuracy score on both the training and testing set are stored to be compared after all evaluation is complete\n",
    "\n",
    ":::{.panel-tabset}\n",
    "\n",
    "### Graph\n",
    "![](figures/output.svg)\n",
    "\n",
    "\n",
    "### Table\n",
    "\n",
    "| ML Model | Training Accuracy | Testing Accuracy |\n",
    "| :------: | :------: | :------: |\n",
    "| Random Forest |\t81.90 |\t82.35 |\n",
    "| Decision Tree |\t81.21 |\t81.80 |\n",
    "| SVM | 79.98 |\t81.15 |\n",
    "| Logistic Regression | 79.81 |\t80.95 |\n",
    "\n",
    ":::\n",
    "\n",
    "Out of all the models the Random Forest had the highest training and testing accuracy  \n",
    "\n",
    "- Training Accuracy of 81.9%\n",
    "- Testing Accuracy of 82.35%\n",
    "\n",
    "Some models like the SVM and Logistic Regression show some slight underfitting with the testing accuracy being around 2% higher than the training accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.notes}\n",
    "Once the data had been split into training and testing sets, each model is then fit on the training set and evaluated on the testing set. Each model's training and testing score is then stored to be compared, resulting in the graph below. As you can see from the table, the random forest classifier had the highest overall training and testing accuracy with {vals}. I also noticed that models like the Linear SVM and the Binary Logistic Regression didn't have particulary balanced training and testing accuracies, which could mean an issue with the fit\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "- Build a new dataset using entries from open-source datacenters like PhishTank\n",
    "- Try deep learning models \n",
    "    - Neural Networks\n",
    "    - Multilayer perceptrons\n",
    "- Build extention based on these models that can classify the website that the user is currently on\n",
    "    - Similar to how Norton labels websites as dangerous with its extention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8e9220ce8a3fa7e2ede426537a02920558b5a3a04901a912ed3e8a06d0fccb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
